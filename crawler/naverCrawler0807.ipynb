{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:34:40.114019Z",
     "start_time": "2019-08-08T13:34:39.168402Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "import datetime\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:17:27.115995Z",
     "start_time": "2019-08-08T14:17:27.113001Z"
    }
   },
   "outputs": [],
   "source": [
    "#추가로 돌릴 경우 이 부분만 불러와서 하면 됨\n",
    "#날짜는 밑에서 기준 날짜 설정 수정하면 문제 없음\n",
    "#초기화 할 경우에는 db 생성 부분 #제거하고 하면 됨\n",
    "#db 생성하는 부분 막 돌리면 절대 안됨 db 다 날아감 그럼 화낼거임\n",
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"naver_news.db\") #news.db생성\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:17:28.137187Z",
     "start_time": "2019-08-08T14:17:28.126183Z"
    }
   },
   "outputs": [],
   "source": [
    "#처음 돌릴때나 db 초기화 할때만 해야됨\n",
    "# 안그러면 기존에 있던 db 다 날아감 #절대 하면 안됨\n",
    "\n",
    "# cur.executescript(\"\"\"\n",
    "#         DROP TABLE IF EXISTS table2;\n",
    "#         CREATE TABLE table2(\n",
    "#         id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         date TEXT NOT NULL,\n",
    "#         title TEXT NOT NULL,\n",
    "#         content TEXT NOT NULL,\n",
    "#         link TEXT NOT NULL\n",
    "#         );\n",
    "# \"\"\")\n",
    "\n",
    "# con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:26.899083Z",
     "start_time": "2019-08-08T13:36:26.893100Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(method,url,param=None,data=None,timeout=1,maxretries=3):\n",
    "    try:\n",
    "        resp=requests.request(method,url,params=param,data=data)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if 500<=e.response.status_code<600 and maxretries>0 :\n",
    "            print(maxretries)\n",
    "            time.sleep(timeout)\n",
    "            download(method,url,param,data,timeout,maxretries-1)\n",
    "            print(\"재시도\")\n",
    "        else:\n",
    "            print(e.reponse.status_code)\n",
    "            print(e.response.reason)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:27.315481Z",
     "start_time": "2019-08-08T13:36:27.289510Z"
    }
   },
   "outputs": [],
   "source": [
    "def news_1page(p_id=264,dates=20190730,pages_num=1):\n",
    "    url=\"https://news.naver.com/main/list.nhn?mode=LS2D\"\n",
    "    param={\n",
    "        'sid2':p_id,\n",
    "        'sid1':100,\n",
    "        'mid':'shm',\n",
    "        'date':dates,\n",
    "        'page':pages_num,\n",
    "    }\n",
    "    html=download('get',url,param=param)\n",
    "    dom=BeautifulSoup(html.text,'lxml')\n",
    "\n",
    "    tk=dom.find_all('td',{\"class\":'content'})[0] .find_all({'a':'href'})  #링크,날짜,페이지 들어간거 찾고 a 그래도 링크,페이지,날짜\n",
    "\n",
    "    #이미지 중복 링크 제거용도 이미지 없는거에 대한 대비책\n",
    "    tktk=[]\n",
    "    for i in range(len(tk)):\n",
    "        try:\n",
    "            len(tk[i].img) #이미지를 뽑아냈을때 길이가 있다는거는 이미지 관련 부분\n",
    "        except:\n",
    "            tktk.append(list(tk[i].attrs.values())) #이미지 아닌 부분 출력\n",
    "            \n",
    "\n",
    "    tktk=[y for x in tktk for y in x] #멀티 리스트 되있는거를 풀어줄때 사용 -> 링크, 페이지랑, 날짜\n",
    "\n",
    "    #필요없는 날짜부분 제거 길이 방식으로 그냥 제거 날짜부분 길이가 1임을 이용했음\n",
    "    tktk2=[]\n",
    "    for i in range(len(tktk)):\n",
    "        if len(tktk[i])>2:\n",
    "            tktk2.append(tktk[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #링크부분 뽑아내기\n",
    "    links=[]\n",
    "    for i in range(len(tktk2)):\n",
    "        if tktk2[i].find('https')>(-1): #find를 이용해서 https가 없으면은 -1 있으면 -1 초과의 값 0~\n",
    "            links.append(tktk2[i])\n",
    "        else:\n",
    "            pass\n",
    "    links2=DataFrame(links,columns=['links'])\n",
    "\n",
    "    #1페이지, 11페이지 21.... 페이지만 뽑아내기\n",
    "    pages=[]\n",
    "    for i in range(len(tktk2)):\n",
    "        if tktk2[i].find('page')>(-1): #find 방식으로 페이지 뽑아내기\n",
    "            pages.append(tktk2[i][-3:]) #여기서 page=11 page=9\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    pages=[i.replace('e=','') for i in pages] #replace를 이용해서 \n",
    "    pages=[i.replace('=','') for i in pages]\n",
    "    pages=[int(s) for s in pages] #int 바꾸려고 해당 부분 사용\n",
    "\n",
    "    pages2=[] #페이지 중복 제거 #11 -> 10 12 13 14 ....\n",
    "    for i in range(len(pages)):\n",
    "        if pages[i]>pages_num: #제일 위에 페이지 넘버 가지고 와서 그 숫자보다 큰 것만 추가\n",
    "            pages2.append(pages[i])\n",
    "        else:\n",
    "            pass\n",
    "    pages=[str(s) for s in pages2] #그냥 숫자값으로 들어가면 페이지 부분에서 오류 생겨서 str로 변경\n",
    "    \n",
    "    return links,pages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:27.889748Z",
     "start_time": "2019-08-08T13:36:27.880798Z"
    }
   },
   "outputs": [],
   "source": [
    "def news_other_page(p_id=264,dates=20190730,pages_num=2):\n",
    "    url=\"https://news.naver.com/main/list.nhn?mode=LS2D\"\n",
    "    param={\n",
    "        'sid2':p_id,\n",
    "        'sid1':100,\n",
    "        'mid':'shm',\n",
    "        'date':dates,\n",
    "        'page':pages_num,\n",
    "    }\n",
    "    html=download('get',url,param=param)\n",
    "    dom=BeautifulSoup(html.text,'lxml')\n",
    "    tk=dom.find_all('td',{\"class\":'content'})[0].find_all({'a':'href'})\n",
    "    \n",
    "    tktk=[]\n",
    "    for i in range(len(tk)):\n",
    "        try:\n",
    "            len(tk[i].img)\n",
    "        except:\n",
    "            tktk.append(list(tk[i].attrs.values()))\n",
    "\n",
    "    tktk=[y for x in tktk for y in x]\n",
    "\n",
    "    tktk2=[]\n",
    "    for i in range(len(tktk)):\n",
    "        if len(tktk[i])>2:\n",
    "            tktk2.append(tktk[i])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    links=[]\n",
    "    for i in range(len(tktk2)):\n",
    "        if tktk2[i].find('https')>(-1):\n",
    "            links.append(tktk2[i])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:28.512270Z",
     "start_time": "2019-08-08T13:36:28.509278Z"
    }
   },
   "outputs": [],
   "source": [
    "# page_total=[]\n",
    "# for i in range(10):\n",
    "#     links2,pages2=news_1page(p_id=265,dates=20190802,pages_num=(10*i)+1)\n",
    "#     page_total.append(pages2)\n",
    "# page_total=[y for x in page_total for y in x]\n",
    "\n",
    "# links_total,pages2=news_1page(p_id=265,dates=20190802,pages_num=1)\n",
    "# links_total=DataFrame(links_total,columns=['links'])\n",
    "\n",
    "# for i in range(2,len(page_total)+2):\n",
    "#     links=DataFrame(news_other_page(p_id=265,dates=20190802,pages_num=i),columns=['links'])\n",
    "#     links_total=pd.concat([links_total,links],axis=0)\n",
    "# links_total=links_total.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:28.924291Z",
     "start_time": "2019-08-08T13:36:28.920271Z"
    }
   },
   "outputs": [],
   "source": [
    "# a = datetime.datetime(2019,8,2)\n",
    "# links_real_total=DataFrame(columns=['links'])\n",
    "# s_id=264\n",
    "\n",
    "# for j in range(3):\n",
    "#     d_time=int((a - datetime.timedelta(days=j)).strftime('%Y%m%d'))\n",
    "#     page_total=[]\n",
    "    \n",
    "#     for i in range(10):\n",
    "#         links2,pages2=news_1page(p_id=s_id,dates=d_time,pages_num=(10*i)+1)\n",
    "#         page_total.append(pages2)\n",
    "#     page_total=[y for x in page_total for y in x]\n",
    "\n",
    "#     links_total,pages2=news_1page(p_id=s_id,dates=d_time,pages_num=1)\n",
    "#     links_total=DataFrame(links_total,columns=['links'])\n",
    "\n",
    "#     for k in range(2,len(page_total)+2):\n",
    "#         links=DataFrame(news_other_page(p_id=s_id,dates=d_time,pages_num=k),columns=['links'])\n",
    "#         links_total=pd.concat([links_total,links],axis=0)\n",
    "#         links_total=links_total.reset_index(drop=True)\n",
    "    \n",
    "#     links_real_total=pd.concat([links_real_total,links_total],axis=0)\n",
    "#     links_real_total=links_real_total.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:59:25.735306Z",
     "start_time": "2019-08-08T13:46:48.186137Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 20190807\n",
      "264 20190806\n",
      "264 20190805\n",
      "264 20190804\n",
      "264 20190803\n",
      "264 20190802\n",
      "264 20190801\n",
      "264 20190731\n",
      "264 20190730\n",
      "264 20190729\n",
      "264 20190728\n",
      "264 20190727\n",
      "264 20190726\n",
      "264 20190725\n",
      "264 20190724\n",
      "264 20190723\n",
      "264 20190722\n",
      "264 20190721\n",
      "264 20190720\n",
      "264 20190719\n",
      "264 20190718\n",
      "264 20190717\n",
      "264 20190716\n",
      "264 20190715\n",
      "264 20190714\n",
      "264 20190713\n",
      "264 20190712\n",
      "264 20190711\n",
      "264 20190710\n",
      "264 20190709\n",
      "265 20190807\n",
      "265 20190806\n",
      "265 20190805\n",
      "265 20190804\n",
      "265 20190803\n",
      "265 20190802\n",
      "265 20190801\n",
      "265 20190731\n",
      "265 20190730\n",
      "265 20190729\n",
      "265 20190728\n",
      "265 20190727\n",
      "265 20190726\n",
      "265 20190725\n",
      "265 20190724\n",
      "265 20190723\n",
      "265 20190722\n",
      "265 20190721\n",
      "265 20190720\n",
      "265 20190719\n",
      "265 20190718\n",
      "265 20190717\n",
      "265 20190716\n",
      "265 20190715\n",
      "265 20190714\n",
      "265 20190713\n",
      "265 20190712\n",
      "265 20190711\n",
      "265 20190710\n",
      "265 20190709\n",
      "266 20190807\n",
      "266 20190806\n",
      "266 20190805\n",
      "266 20190804\n",
      "266 20190803\n",
      "266 20190802\n",
      "266 20190801\n",
      "266 20190731\n",
      "266 20190730\n",
      "266 20190729\n",
      "266 20190728\n",
      "266 20190727\n",
      "266 20190726\n",
      "266 20190725\n",
      "266 20190724\n",
      "266 20190723\n",
      "266 20190722\n",
      "266 20190721\n",
      "266 20190720\n",
      "266 20190719\n",
      "266 20190718\n",
      "266 20190717\n",
      "266 20190716\n",
      "266 20190715\n",
      "266 20190714\n",
      "266 20190713\n",
      "266 20190712\n",
      "266 20190711\n",
      "266 20190710\n",
      "266 20190709\n",
      "267 20190807\n",
      "267 20190806\n",
      "267 20190805\n",
      "267 20190804\n",
      "267 20190803\n",
      "267 20190802\n",
      "267 20190801\n",
      "267 20190731\n",
      "267 20190730\n",
      "267 20190729\n",
      "267 20190728\n",
      "267 20190727\n",
      "267 20190726\n",
      "267 20190725\n",
      "267 20190724\n",
      "267 20190723\n",
      "267 20190722\n",
      "267 20190721\n",
      "267 20190720\n",
      "267 20190719\n",
      "267 20190718\n",
      "267 20190717\n",
      "267 20190716\n",
      "267 20190715\n",
      "267 20190714\n",
      "267 20190713\n",
      "267 20190712\n",
      "267 20190711\n",
      "267 20190710\n",
      "267 20190709\n",
      "268 20190807\n",
      "268 20190806\n",
      "268 20190805\n",
      "268 20190804\n",
      "268 20190803\n",
      "268 20190802\n",
      "268 20190801\n",
      "268 20190731\n",
      "268 20190730\n",
      "268 20190729\n",
      "268 20190728\n",
      "268 20190727\n",
      "268 20190726\n",
      "268 20190725\n",
      "268 20190724\n",
      "268 20190723\n",
      "268 20190722\n",
      "268 20190721\n",
      "268 20190720\n",
      "268 20190719\n",
      "268 20190718\n",
      "268 20190717\n",
      "268 20190716\n",
      "268 20190715\n",
      "268 20190714\n",
      "268 20190713\n",
      "268 20190712\n",
      "268 20190711\n",
      "268 20190710\n",
      "268 20190709\n",
      "269 20190807\n",
      "269 20190806\n",
      "269 20190805\n",
      "269 20190804\n",
      "269 20190803\n",
      "269 20190802\n",
      "269 20190801\n",
      "269 20190731\n",
      "269 20190730\n",
      "269 20190729\n",
      "269 20190728\n",
      "269 20190727\n",
      "269 20190726\n",
      "269 20190725\n",
      "269 20190724\n",
      "269 20190723\n",
      "269 20190722\n",
      "269 20190721\n",
      "269 20190720\n",
      "269 20190719\n",
      "269 20190718\n",
      "269 20190717\n",
      "269 20190716\n",
      "269 20190715\n",
      "269 20190714\n",
      "269 20190713\n",
      "269 20190712\n",
      "269 20190711\n",
      "269 20190710\n",
      "269 20190709\n"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime(2019,8,7) #원하는 날짜 지정 #30일 기준으로 만들었으니 다음에 추가할때는 7,8로 시작하면 됨\n",
    "links_real_total=DataFrame(columns=['links']) #링크 빈 data frame\n",
    "\n",
    "for m in range(264,270): #264~269 총 6번\n",
    "    for j in range(30): #원하는 날짜 주기 # 1넣으면 당일만 #3 넣으면은 8/2일 기준 -> 8/2, 8/1 7/31 \n",
    "        d_time=int((a - datetime.timedelta(days=j)).strftime('%Y%m%d')) #기준 날짜에서 빼서 하는거 8/2 -> 8/1 \n",
    "        page_total=[] #빈 페이지 번호\n",
    "\n",
    "        #해당 카테코리 부분의 페이지 번호 전체 가져오기\n",
    "        for i in range(20): #여유있게 지정해둔 것\n",
    "            links2,pages2=news_1page(p_id=m,dates=d_time,pages_num=(10*i)+1) #1페이지 -> 11페이지 -> 21 -> 31 .....\n",
    "            page_total.append(pages2) #페이지가 동일하게 처리 되는 시점 부터는 최종 페이지까지만 나오게 됨\n",
    "        page_total=[y for x in page_total for y in x] #멀티 리스트 처리\n",
    "\n",
    "        links_total,pages2=news_1page(p_id=m,dates=d_time,pages_num=1) #링크 처음부터 넣는 부분\n",
    "        links_total=DataFrame(links_total,columns=['links']) #1페이지에 있는 링크 dataframe\n",
    " \n",
    "        #2페이지부터의 과정\n",
    "        for k in range(2,len(page_total)+2):\n",
    "            links=DataFrame(news_other_page(p_id=m,dates=d_time,pages_num=k),columns=['links']) #2페이지 짜리는 바로 링크만 있으니 바로 dataframe\n",
    "            links_total=pd.concat([links_total,links],axis=0) #concat 해서 dataframe 바로 밑으로 합치기 \n",
    "            links_total=links_total.reset_index(drop=True)\n",
    "\n",
    "        links_real_total=pd.concat([links_real_total,links_total],axis=0)\n",
    "        links_real_total=links_real_total.reset_index(drop=True)\n",
    "        print(m,d_time) #중간에 멈추면 그 부분부터 다시 돌리면 됨 #돌리는건 알아서 수정해야되는게 단점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:15:49.458332Z",
     "start_time": "2019-08-08T14:15:49.454343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95316"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links_real_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:17:54.003027Z",
     "start_time": "2019-08-08T14:17:53.532627Z"
    }
   },
   "outputs": [],
   "source": [
    "links_real_total.to_sql('table1', con) #dataframe 통째로 db에 넣는 방법 #sqlite.sequence에서 숫자를 안 세줌\n",
    "\n",
    "# links_real_total.to_sql('table1', con, if_exists='append') #만약에 추가로 넣어야될때 사용해야되는 방법, 그냥 넣으면 작동 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30일치 링크만 뽑는데          12분 30초\n",
    "30일치 링크만 db에 집어넣는데 1초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T09:28:37.799398Z",
     "start_time": "2019-08-08T09:28:12.107860Z"
    }
   },
   "outputs": [],
   "source": [
    "# link=[]\n",
    "# date=[]\n",
    "# title=[]\n",
    "# write=[]\n",
    "\n",
    "# # for i in range(len(links_real_total)):\n",
    "# for i in range(2600,2700):\n",
    "#     html=download('get',links_real_total['links'][i])\n",
    "#     dom=BeautifulSoup(html.text,'lxml')\n",
    "#     time.sleep(0.1)\n",
    "#     try:\n",
    "#         oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도\n",
    "#     except:\n",
    "#         pass\n",
    "#     else:\n",
    "#         link.append(links_real_total['links'][i])\n",
    "#         #date 입력되는 길이 이용해서 걸러내는 방법\n",
    "#         date.append(oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25])\n",
    "\n",
    "#         #title 찾고 추가로 들어가있는 부분 대체하는 방법\n",
    "#         title.append(dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"))\n",
    "\n",
    "#         #본문 일단 다 가져오는 방법 #함수 제거하는 것과 광고 부분을 길이를 이용해서 제거해버림\n",
    "#         o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text\n",
    "#         write.append(o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:37:00.983760Z",
     "start_time": "2019-08-08T14:20:11.637620Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(links_real_total)):\n",
    "    html=download('get',links_real_total['links'][i])\n",
    "    dom=BeautifulSoup(html.text,'lxml')\n",
    "    time.sleep(0.2) #timeout error 방지용 최소마지노선\n",
    "    try:\n",
    "        oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도 #정치기사 아닌거 돌리면 무조건 에러남\n",
    "    except:\n",
    "        pass #정치기사 아닌건 db에 안 넣을 거임\n",
    "    else:\n",
    "        o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text #기사 내용 걸러내기 1차, 밑에는 2차 걸러내기\n",
    "        content=o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')].replace('\\xa0','').replace(\"\\'\",'').replace('\\t','').replace('{}\\n\\n','')\n",
    "        if dom.find('title').text.replace(\" : 네이버 뉴스\",\"\").find('속보')>(-1) or len(content)<350: #속보인거랑 길이 350가 안되는 사진기사나 간략 기사는 걸러낼거임\n",
    "            pass\n",
    "        else:\n",
    "            date=oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25].replace('\\n','').replace('\\r','')\n",
    "            #날짜 부분은 길이를 이용해서 추가하고 삐져나온 부분만 제거해주는 방식으로 함\n",
    "            \n",
    "            #주어진 table에다가 넣는 방식으로 하고 이후에 db에서 골라낸 뒤에 다시 집어넣는 방식을 사용하는 것을 생각하고 있음\n",
    "            cur.execute(\"\"\"\n",
    "            INSERT INTO table2\n",
    "            (date, title, content, link)\n",
    "            VALUES(?,?,?,?)\n",
    "            \"\"\", [date, dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"), content, links_real_total['links'][i]] )\n",
    "            con.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하루치 돌아가는데 4900개 기준 28분\n",
    "\n",
    "3개 돌아가는데 1초 -> 180개 돌아가는데 1분 \n",
    "\n",
    "30일 돌아가는데 예상시간 8시간 -> 실제시간 9시간 17분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T09:48:33.682686Z",
     "start_time": "2019-08-08T09:48:10.545355Z"
    }
   },
   "outputs": [],
   "source": [
    "# # for i in range(len(links_real_total)):\n",
    "# for i in range(2600,2700):\n",
    "#     html=download('get',links_real_total['links'][i])\n",
    "#     dom=BeautifulSoup(html.text,'lxml')\n",
    "#     time.sleep(0.1)\n",
    "#     try:\n",
    "#         oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도 #정치기사 아닌거 돌리면 무조건 에러남\n",
    "#     except:\n",
    "#         pass #정치기사 아닌건 db에 안 넣을음\n",
    "#     else:\n",
    "#         date=oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25].replace('\\n','').replace('\\r','')\n",
    "        \n",
    "#         o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text\n",
    "#         content=o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')].replace('\\xa0','').replace(\"\\'\",'').replace('\\t','')\n",
    "        \n",
    "#         cur.execute(\"\"\"\n",
    "#         INSERT INTO table1\n",
    "#         (date, title, content, link)\n",
    "#         VALUES(?,?,?,?)\n",
    "#         \"\"\", [date, dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"), content, links_real_total['links'][i]] )\n",
    "#         con.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:45:22.795296Z",
     "start_time": "2019-08-08T23:45:22.790302Z"
    }
   },
   "outputs": [],
   "source": [
    "# con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
