{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:34:40.114019Z",
     "start_time": "2019-08-08T13:34:39.168402Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "import datetime\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:17:27.115995Z",
     "start_time": "2019-08-08T14:17:27.113001Z"
    }
   },
   "outputs": [],
   "source": [
    "#추가로 돌릴 경우 이 부분만 불러와서 하면 됨\n",
    "#날짜는 밑에서 기준 날짜 설정 수정하면 문제 없음\n",
    "#초기화 할 경우에는 db 생성 부분 #제거하고 하면 됨\n",
    "#db 생성하는 부분 막 돌리면 절대 안됨 db 다 날아감 그럼 화낼거임\n",
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"naver_news.db\") #news.db생성\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:17:28.137187Z",
     "start_time": "2019-08-08T14:17:28.126183Z"
    }
   },
   "outputs": [],
   "source": [
    "#처음 돌릴때나 db 초기화 할때만 해야됨\n",
    "# 안그러면 기존에 있던 db 다 날아감 #절대 하면 안됨\n",
    "\n",
    "# cur.executescript(\"\"\"\n",
    "#         DROP TABLE IF EXISTS table2;\n",
    "#         CREATE TABLE table2(\n",
    "#         id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "#         date TEXT NOT NULL,\n",
    "#         title TEXT NOT NULL,\n",
    "#         content TEXT NOT NULL,\n",
    "#         link TEXT NOT NULL\n",
    "#         );\n",
    "# \"\"\")\n",
    "\n",
    "# con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:26.899083Z",
     "start_time": "2019-08-08T13:36:26.893100Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(method,url,param=None,data=None,timeout=1,maxretries=3):\n",
    "    try:\n",
    "        resp=requests.request(method,url,params=param,data=data)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if 500<=e.response.status_code<600 and maxretries>0 :\n",
    "            print(maxretries)\n",
    "            time.sleep(timeout)\n",
    "            download(method,url,param,data,timeout,maxretries-1)\n",
    "            print(\"재시도\")\n",
    "        else:\n",
    "            print(e.reponse.status_code)\n",
    "            print(e.response.reason)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:27.315481Z",
     "start_time": "2019-08-08T13:36:27.289510Z"
    }
   },
   "outputs": [],
   "source": [
    "def news_1page(p_id=264,dates=20190730,pages_num=1):\n",
    "    url=\"https://news.naver.com/main/list.nhn?mode=LS2D\"\n",
    "    param={\n",
    "        'sid2':p_id,\n",
    "        'sid1':100,\n",
    "        'mid':'shm',\n",
    "        'date':dates,\n",
    "        'page':pages_num,\n",
    "    }\n",
    "    html=download('get',url,param=param)\n",
    "    dom=BeautifulSoup(html.text,'lxml')\n",
    "\n",
    "    tk=dom.find_all('td',{\"class\":'content'})[0] .find_all({'a':'href'})  #링크,날짜,페이지 들어간거 찾고 a 그래도 링크,페이지,날짜\n",
    "\n",
    "    #이미지 중복 링크 제거용도 이미지 없는거에 대한 대비책\n",
    "    tktk=[]\n",
    "    for i in range(len(tk)):\n",
    "        try:\n",
    "            len(tk[i].img) #이미지를 뽑아냈을때 길이가 있다는거는 이미지 관련 부분\n",
    "        except:\n",
    "            tktk.append(list(tk[i].attrs.values())) #이미지 아닌 부분 출력\n",
    "            \n",
    "\n",
    "    tktk=[y for x in tktk for y in x] #멀티 리스트 되있는거를 풀어줄때 사용 -> 링크, 페이지랑, 날짜\n",
    "\n",
    "    #필요없는 날짜부분 제거 길이 방식으로 그냥 제거 날짜부분 길이가 1임을 이용했음\n",
    "    tktk2=[]\n",
    "    for i in range(len(tktk)):\n",
    "        if len(tktk[i])>2:\n",
    "            tktk2.append(tktk[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #링크부분 뽑아내기\n",
    "    links=[]\n",
    "    for i in range(len(tktk2)):\n",
    "        if tktk2[i].find('https')>(-1): #find를 이용해서 https가 없으면은 -1 있으면 -1 초과의 값 0~\n",
    "            links.append(tktk2[i])\n",
    "        else:\n",
    "            pass\n",
    "    links2=DataFrame(links,columns=['links'])\n",
    "\n",
    "    #1페이지, 11페이지 21.... 페이지만 뽑아내기\n",
    "    pages=[]\n",
    "    for i in range(len(tktk2)):\n",
    "        if tktk2[i].find('page')>(-1): #find 방식으로 페이지 뽑아내기\n",
    "            pages.append(tktk2[i][-3:]) #여기서 page=11 page=9\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    pages=[i.replace('e=','') for i in pages] #replace를 이용해서 \n",
    "    pages=[i.replace('=','') for i in pages]\n",
    "    pages=[int(s) for s in pages] #int 바꾸려고 해당 부분 사용\n",
    "\n",
    "    pages2=[] #페이지 중복 제거 #11 -> 10 12 13 14 ....\n",
    "    for i in range(len(pages)):\n",
    "        if pages[i]>pages_num: #제일 위에 페이지 넘버 가지고 와서 그 숫자보다 큰 것만 추가\n",
    "            pages2.append(pages[i])\n",
    "        else:\n",
    "            pass\n",
    "    pages=[str(s) for s in pages2] #그냥 숫자값으로 들어가면 페이지 부분에서 오류 생겨서 str로 변경\n",
    "    \n",
    "    return links,pages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:27.889748Z",
     "start_time": "2019-08-08T13:36:27.880798Z"
    }
   },
   "outputs": [],
   "source": [
    "def news_other_page(p_id=264,dates=20190730,pages_num=2):\n",
    "    url=\"https://news.naver.com/main/list.nhn?mode=LS2D\"\n",
    "    param={\n",
    "        'sid2':p_id,\n",
    "        'sid1':100,\n",
    "        'mid':'shm',\n",
    "        'date':dates,\n",
    "        'page':pages_num,\n",
    "    }\n",
    "    html=download('get',url,param=param)\n",
    "    dom=BeautifulSoup(html.text,'lxml')\n",
    "    tk=dom.find_all('td',{\"class\":'content'})[0].find_all({'a':'href'})\n",
    "    \n",
    "    tktk=[]\n",
    "    for i in range(len(tk)):\n",
    "        try:\n",
    "            len(tk[i].img)\n",
    "        except:\n",
    "            tktk.append(list(tk[i].attrs.values()))\n",
    "\n",
    "    tktk=[y for x in tktk for y in x]\n",
    "\n",
    "    tktk2=[]\n",
    "    for i in range(len(tktk)):\n",
    "        if len(tktk[i])>2:\n",
    "            tktk2.append(tktk[i])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    links=[]\n",
    "    for i in range(len(tktk2)):\n",
    "        if tktk2[i].find('https')>(-1):\n",
    "            links.append(tktk2[i])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:28.512270Z",
     "start_time": "2019-08-08T13:36:28.509278Z"
    }
   },
   "outputs": [],
   "source": [
    "# page_total=[]\n",
    "# for i in range(10):\n",
    "#     links2,pages2=news_1page(p_id=265,dates=20190802,pages_num=(10*i)+1)\n",
    "#     page_total.append(pages2)\n",
    "# page_total=[y for x in page_total for y in x]\n",
    "\n",
    "# links_total,pages2=news_1page(p_id=265,dates=20190802,pages_num=1)\n",
    "# links_total=DataFrame(links_total,columns=['links'])\n",
    "\n",
    "# for i in range(2,len(page_total)+2):\n",
    "#     links=DataFrame(news_other_page(p_id=265,dates=20190802,pages_num=i),columns=['links'])\n",
    "#     links_total=pd.concat([links_total,links],axis=0)\n",
    "# links_total=links_total.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:36:28.924291Z",
     "start_time": "2019-08-08T13:36:28.920271Z"
    }
   },
   "outputs": [],
   "source": [
    "# a = datetime.datetime(2019,8,2)\n",
    "# links_real_total=DataFrame(columns=['links'])\n",
    "# s_id=264\n",
    "\n",
    "# for j in range(3):\n",
    "#     d_time=int((a - datetime.timedelta(days=j)).strftime('%Y%m%d'))\n",
    "#     page_total=[]\n",
    "    \n",
    "#     for i in range(10):\n",
    "#         links2,pages2=news_1page(p_id=s_id,dates=d_time,pages_num=(10*i)+1)\n",
    "#         page_total.append(pages2)\n",
    "#     page_total=[y for x in page_total for y in x]\n",
    "\n",
    "#     links_total,pages2=news_1page(p_id=s_id,dates=d_time,pages_num=1)\n",
    "#     links_total=DataFrame(links_total,columns=['links'])\n",
    "\n",
    "#     for k in range(2,len(page_total)+2):\n",
    "#         links=DataFrame(news_other_page(p_id=s_id,dates=d_time,pages_num=k),columns=['links'])\n",
    "#         links_total=pd.concat([links_total,links],axis=0)\n",
    "#         links_total=links_total.reset_index(drop=True)\n",
    "    \n",
    "#     links_real_total=pd.concat([links_real_total,links_total],axis=0)\n",
    "#     links_real_total=links_real_total.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:59:25.735306Z",
     "start_time": "2019-08-08T13:46:48.186137Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 20190809\n",
      "265 20190809\n",
      "266 20190809\n",
      "267 20190809\n",
      "268 20190809\n",
      "269 20190809\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "near \"'{d_time}'\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-17ced1c46c2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mlink\u001b[0m \u001b[0mTEXT\u001b[0m \u001b[0mNOT\u001b[0m \u001b[0mNULL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         );\n\u001b[1;32m---> 36\u001b[1;33m     \"\"\")\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: near \"'{d_time}'\": syntax error"
     ]
    }
   ],
   "source": [
    "a = datetime.datetime(2019,8,9) #원하는 날짜 지정 #30일 기준으로 만들었으니 다음에 추가할때는 7,8로 시작하면 됨\n",
    "links_real_total=DataFrame(columns=['links']) #링크 빈 data frame\n",
    "\n",
    "for j in range(2): #264~269 총 6번 \n",
    "    for m in range(264,270): #원하는 날짜 주기 # 1넣으면 당일만 #3 넣으면은 8/2일 기준 -> 8/2, 8/1 7/31 \n",
    "        d_time=int((a - datetime.timedelta(days=j)).strftime('%Y%m%d')) #기준 날짜에서 빼서 하는거 8/2 -> 8/1 \n",
    "        page_total=[] #빈 페이지 번호\n",
    "\n",
    "        #해당 카테코리 부분의 페이지 번호 전체 가져오기\n",
    "        for i in range(20): #여유있게 지정해둔 것\n",
    "            links2,pages2=news_1page(p_id=m,dates=d_time,pages_num=(10*i)+1) #1페이지 -> 11페이지 -> 21 -> 31 .....\n",
    "            page_total.append(pages2) #페이지가 동일하게 처리 되는 시점 부터는 최종 페이지까지만 나오게 됨\n",
    "        page_total=[y for x in page_total for y in x] #멀티 리스트 처리\n",
    "\n",
    "        links_total,pages2=news_1page(p_id=m,dates=d_time,pages_num=1) #링크 처음부터 넣는 부분\n",
    "        links_total=DataFrame(links_total,columns=['links']) #1페이지에 있는 링크 dataframe\n",
    " \n",
    "        #2페이지부터의 과정\n",
    "        for k in range(2,len(page_total)+2):\n",
    "            links=DataFrame(news_other_page(p_id=m,dates=d_time,pages_num=k),columns=['links']) #2페이지 짜리는 바로 링크만 있으니 바로 dataframe\n",
    "            links_total=pd.concat([links_total,links],axis=0) #concat 해서 dataframe 바로 밑으로 합치기 \n",
    "            links_total=links_total.reset_index(drop=True)\n",
    "\n",
    "        links_real_total=pd.concat([links_real_total,links_total],axis=0)\n",
    "        links_real_total=links_real_total.reset_index(drop=True)\n",
    "        print(m,d_time) #중간에 멈추면 그 부분부터 다시 돌리면 됨 #돌리는건 알아서 수정해야되는게 단점\n",
    "    \n",
    "    cur.executescript('''\n",
    "        CREATE TABLE {}(\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "        date TEXT NOT NULL,\n",
    "        title TEXT NOT NULL,\n",
    "        content TEXT NOT NULL,\n",
    "        link TEXT NOT NULL\n",
    "        );\n",
    "    '''.format(d_time))\n",
    "\n",
    "    con.commit()\n",
    "    \n",
    "    for i in range(len(links_real_total)):\n",
    "        html=download('get',links_real_total['links'][i])\n",
    "        dom=BeautifulSoup(html.text,'lxml')\n",
    "        time.sleep(0.2) #timeout error 방지용 최소마지노선\n",
    "        try:\n",
    "            oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도 #정치기사 아닌거 돌리면 무조건 에러남\n",
    "        except:\n",
    "            pass #정치기사 아닌건 db에 안 넣을 거임\n",
    "        else:\n",
    "            o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text #기사 내용 걸러내기 1차, 밑에는 2차 걸러내기\n",
    "            content=o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')].replace('\\xa0','').replace(\"\\'\",'').replace('\\t','').replace('{}\\n\\n','')\n",
    "            if dom.find('title').text.replace(\" : 네이버 뉴스\",\"\").find('속보')>(-1) or len(content)<350: #속보인거랑 길이 350가 안되는 사진기사나 간략 기사는 걸러낼거임\n",
    "                pass\n",
    "            else:\n",
    "                date=oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25].replace('\\n','').replace('\\r','')\n",
    "                #날짜 부분은 길이를 이용해서 추가하고 삐져나온 부분만 제거해주는 방식으로 함\n",
    "\n",
    "                #주어진 table에다가 넣는 방식으로 하고 이후에 db에서 골라낸 뒤에 다시 집어넣는 방식을 사용하는 것을 생각하고 있음\n",
    "                cur.execute(\"\"\"\n",
    "                INSERT INTO d_time\n",
    "                (date, title, content, link)\n",
    "                VALUES(?,?,?,?)\n",
    "                \"\"\", [date, dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"), content, links_real_total['links'][i]] )\n",
    "                con.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190809'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"\"\"{d_time}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executescript(f\"\"\"\n",
    "        CREATE TABLE db{d_time}(\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "        date TEXT NOT NULL,\n",
    "        title TEXT NOT NULL,\n",
    "        content TEXT NOT NULL,\n",
    "        link TEXT NOT NULL\n",
    "        );\"\"\")\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:15:49.458332Z",
     "start_time": "2019-08-08T14:15:49.454343Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links_real_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20190808"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=100&sid2=269&oid=009&aid=0004406288'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_real_total['links'][6999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T14:17:54.003027Z",
     "start_time": "2019-08-08T14:17:53.532627Z"
    }
   },
   "outputs": [],
   "source": [
    "# links_real_total.to_sql('table1', con) #dataframe 통째로 db에 넣는 방법 #sqlite.sequence에서 숫자를 안 세줌\n",
    "\n",
    "links_real_total.to_sql(f'{d_time}', con, if_exists='append') #만약에 추가로 넣어야될때 사용해야되는 방법, 그냥 넣으면 작동 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30일치 링크만 뽑는데          12분 30초\n",
    "30일치 링크만 db에 집어넣는데 1초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T09:28:37.799398Z",
     "start_time": "2019-08-08T09:28:12.107860Z"
    }
   },
   "outputs": [],
   "source": [
    "# link=[]\n",
    "# date=[]\n",
    "# title=[]\n",
    "# write=[]\n",
    "\n",
    "# # for i in range(len(links_real_total)):\n",
    "# for i in range(2600,2700):\n",
    "#     html=download('get',links_real_total['links'][i])\n",
    "#     dom=BeautifulSoup(html.text,'lxml')\n",
    "#     time.sleep(0.1)\n",
    "#     try:\n",
    "#         oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도\n",
    "#     except:\n",
    "#         pass\n",
    "#     else:\n",
    "#         link.append(links_real_total['links'][i])\n",
    "#         #date 입력되는 길이 이용해서 걸러내는 방법\n",
    "#         date.append(oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25])\n",
    "\n",
    "#         #title 찾고 추가로 들어가있는 부분 대체하는 방법\n",
    "#         title.append(dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"))\n",
    "\n",
    "#         #본문 일단 다 가져오는 방법 #함수 제거하는 것과 광고 부분을 길이를 이용해서 제거해버림\n",
    "#         o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text\n",
    "#         write.append(o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:37:00.983760Z",
     "start_time": "2019-08-08T14:20:11.637620Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(links_real_total)):\n",
    "    html=download('get',links_real_total['links'][i])\n",
    "    dom=BeautifulSoup(html.text,'lxml')\n",
    "    time.sleep(0.2) #timeout error 방지용 최소마지노선\n",
    "    try:\n",
    "        oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도 #정치기사 아닌거 돌리면 무조건 에러남\n",
    "    except:\n",
    "        pass #정치기사 아닌건 db에 안 넣을 거임\n",
    "    else:\n",
    "        o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text #기사 내용 걸러내기 1차, 밑에는 2차 걸러내기\n",
    "        content=o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')].replace('\\xa0','').replace(\"\\'\",'').replace('\\t','').replace('{}\\n\\n','')\n",
    "        if dom.find('title').text.replace(\" : 네이버 뉴스\",\"\").find('속보')>(-1) or len(content)<350: #속보인거랑 길이 350가 안되는 사진기사나 간략 기사는 걸러낼거임\n",
    "            pass\n",
    "        else:\n",
    "            date=oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25].replace('\\n','').replace('\\r','')\n",
    "            #날짜 부분은 길이를 이용해서 추가하고 삐져나온 부분만 제거해주는 방식으로 함\n",
    "            \n",
    "            #주어진 table에다가 넣는 방식으로 하고 이후에 db에서 골라낸 뒤에 다시 집어넣는 방식을 사용하는 것을 생각하고 있음\n",
    "            cur.execute(\"\"\"\n",
    "            INSERT INTO table2\n",
    "            (date, title, content, link)\n",
    "            VALUES(?,?,?,?)\n",
    "            \"\"\", [date, dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"), content, links_real_total['links'][i]] )\n",
    "            con.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하루치 돌아가는데 4900개 기준 28분\n",
    "\n",
    "3개 돌아가는데 1초 -> 180개 돌아가는데 1분 \n",
    "\n",
    "30일 돌아가는데 예상시간 8시간 -> 실제시간 9시간 17분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T09:48:33.682686Z",
     "start_time": "2019-08-08T09:48:10.545355Z"
    }
   },
   "outputs": [],
   "source": [
    "# # for i in range(len(links_real_total)):\n",
    "# for i in range(2600,2700):\n",
    "#     html=download('get',links_real_total['links'][i])\n",
    "#     dom=BeautifulSoup(html.text,'lxml')\n",
    "#     time.sleep(0.1)\n",
    "#     try:\n",
    "#         oo=dom.find('div',{'class':'sponsor'}).text #정치 기사가 아닌 텍스트 골라내기 용도 #정치기사 아닌거 돌리면 무조건 에러남\n",
    "#     except:\n",
    "#         pass #정치기사 아닌건 db에 안 넣을음\n",
    "#     else:\n",
    "#         date=oo[dom.find('div',{'class':'sponsor'}).text.find('기사입력')+5:dom.find('div',{'class':'sponsor'}).text.find('기사입력')+25].replace('\\n','').replace('\\r','')\n",
    "        \n",
    "#         o2=dom.find_all('div',{'id':'articleBodyContents'})[0].text\n",
    "#         content=o2[dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('()')+3:dom.find_all('div',{'id':'articleBodyContents'})[0].text.find('▶')].replace('\\xa0','').replace(\"\\'\",'').replace('\\t','')\n",
    "        \n",
    "#         cur.execute(\"\"\"\n",
    "#         INSERT INTO table1\n",
    "#         (date, title, content, link)\n",
    "#         VALUES(?,?,?,?)\n",
    "#         \"\"\", [date, dom.find('title').text.replace(\" : 네이버 뉴스\",\"\"), content, links_real_total['links'][i]] )\n",
    "#         con.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T23:45:22.795296Z",
     "start_time": "2019-08-08T23:45:22.790302Z"
    }
   },
   "outputs": [],
   "source": [
    "# con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
